{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zb0Tr85SYAd5"
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "In this labs, we are going to apply linear regression to the problem of predicting developer satisfaction based upon information about their careers, from a StackOverflow survey.  The data from this question is based on the [2019 StackOverflow Survey](https://insights.stackoverflow.com/survey/2019); accordingly, the subset bundled with this assignment is also released under the Open Database License (ODbL) v1.0.  For this problem, you should not use Scikit-Learn, but instead, implement all the least squares solutions manually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsSSM7jXYAeC"
   },
   "source": [
    "### Q1 Data Parsing\n",
    "\n",
    "The data from this survey has the values as described below.  Your eventual goal will be to predict (a numerical equivalent of) the `CarreerSat` column, indicating how satisfied the subject is with their career), based upon the other values.  Note that because the career satisfaction values are ordinal, we likely should not be predicting them with linear regression, but as an illustrative example, this is still a reasonable task.\n",
    "\n",
    "The data set contains the following columns.\n",
    "\n",
    "| Column | Sample | Does/is the respondent... | Type/Values |\n",
    "| --- |:--- |:--- |:--- |\n",
    "| **CareerSat** | 'vs' | satisfied with their career? | (`vd`, `sd`, `ne`, `NA`, `ss`, `vs`) -- corresponding to ({very, slightly}, {satisfied, dissatisfied}), neutral, and not applicable |\n",
    "| MgrWant | 'n' | ...want to be a manager? | boolean |\n",
    "| Age    | '22'   | age | integer     |\n",
    "| CodeRevHrs | '2' | hours a week spent reviewing code | integer |\n",
    "| ConvertedComp | '61000' | yearly compensation in 2019 USD | integer |\n",
    "| Country | 'United States' | lives in country | string _(ignore in regression)_ |\n",
    "| Dependents | 'n' | ...have children or other dependents. | boolean |\n",
    "| DevEnvironVSC | 'y' | ...use Visual Studio Code | boolean |\n",
    "| DevTypeFullStack | 'n' | ...identify as a full-stack developer | boolean |\n",
    "| EdLevel | 'bachelors' | maximum education level | (`other`, `bachelors`, `masters`, `doctoral`) |\n",
    "| EduOtherMOOC | 'y' | ...ever taken a Massively Open Online Course | boolean |\n",
    "| EduOtherSelf | 'y' | ...ever taught themselves a new platform | boolean _(ignore in regression, this is  duplicate field)_ |\n",
    "| Extraversion | 'y' | ...prefer in-person meetings to online meetings | boolean |\n",
    "| GenderIsMan | 'y' | ...male | boolean |\n",
    "| Hobbyist | 'n' | ...write code as a hobby? | boolean |\n",
    "| MgrIdiot | 'very' | ...think their manager knows what they are doing? | (`NA`, `not`, `some`, `very`), in order of increasing confidence |\n",
    "| OpSys | 'win' | which OS do they use? | (`win`, `mac`, `tux`, `NA`), for (Windows, Mac OSX, Linux-like, NA) |\n",
    "| OpenSourcer | 'Never' | ...contribute to open-source projects? | (`never`, `year`, `month-year`, `month`), in increasing order of frequency |\n",
    "| OrgSize | '100-499' | number of employees in organization? | (`NA`, `1`, `2-9`, `10-19`, `20-99`, `100-499`, `500-999`, `1,000-4,999`, `5,000-9,999`, `10,000+`) |\n",
    "| Respondent | '4' | respondent ID from original data | integer _(ignore in regression)_ |\n",
    "| Student | 'n' | ...currently a student? | boolean |\n",
    "| UndergradMajorIsComputerScience | 'y' | ...majored in CS? | boolean |\n",
    "| UnitTestsProcess | 'n' | ...use unit tests in their job? | boolean |\n",
    "| WorkWeekHrs | '40' | hours a week worked | integer |\n",
    "| YearsCode | 3 | years since first programming | integer |\n",
    "| YearsCodePro | 0 | years programming professionally | integer |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIaLMH-GYAeG"
   },
   "source": [
    "When you load the data for linear regression, you will want to convert each type to a floating point value according to the columns above.  Specifically, for each column listed above, convert the value to a numeric quantity using the rules below.  Note that there are some fairly unnatural assumptions here (like converting NAs to 0.0), but these are largely to illustrate the methodology without adding too much additional processing work.\n",
    "\n",
    " - _boolean_ : `y`/`NA`/`n` assigned to `+1.0`/`0.0`/`0.0`\n",
    " - _integer_ : convert to `float`, preserving value. `NA` equals `0.0`. \n",
    " - _string_ : not included in regression; we'll use it later\n",
    " - CareerSat: Map (`vd`, `sd`, `ne`, `NA`, `ss`, `vs`) to (-2.0, -1.0, 0.0, 0.0, 1.0, 2.0)\n",
    " - EdLevel: Map (`other`, `bachelors`, `masters`, `doctoral`) to (0.0, 1.0, 1.5, 2.0)\n",
    " - MgrIdiot: Map (`NA`, `not`, `some`, `very`) to (-1.0, -1.0, 0.0, 1.0)\n",
    " - OpSys: Map (`win`, `mac`, `NA`, `tux`, `BSD`) to (-1.0, 0.0, 0.0, 1.0, 1.0)\n",
    " - OpenSourcer : Map (`never`, `year`, `month-year`, `month`) to (0.0, 0.5, 1.0, 2.0)\n",
    " - OrgSize: Map each range \"$a$-$b$\" to the value $ln(a)$. Treat `NA` as `ln(1.0) = 0`. We are converting an exponentially distributed range to a linearly distributed one.\n",
    "\n",
    "Remove the columns listed above as being ignored. \n",
    "\n",
    "Some hints:\n",
    "  1. Load the csv file with `pd.read_csv(filname, dtype=str, keep_default_na=False)` to ensure that you load all columns as text (so you can do your own preprocessing), and ignore pandas's default conversion to NaN values.\n",
    "  2. Use the `.apply()` function in pandas to convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLu0XmT1YAeI"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import math\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uzLmJbJYAeK"
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_stackoverflow_data(filename=\"eggs.csv.gz\"):\n",
    "    \"\"\"\n",
    "    Load data from the given file, and convert for use in linear regression.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the CSV file (gzip compressed).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with the data converted to floating-point values appropriately.\n",
    "    \"\"\"\n",
    "    # Load the data as strings, prevent automatic NA conversion\n",
    "    data = pd.read_csv(filename, dtype=str, keep_default_na=False)\n",
    "    \n",
    "    # Mappings for categorical and ordinal data\n",
    "    career_sat_map = {'vd': -2.0, 'sd': -1.0, 'ne': 0.0, 'NA': 0.0, 'ss': 1.0, 'vs': 2.0}\n",
    "    ed_level_map = {'other': 0.0, 'bachelors': 1.0, 'masters': 1.5, 'doctoral': 2.0}\n",
    "    mgr_idiot_map = {'NA': -1.0, 'not': -1.0, 'some': 0.0, 'very': 1.0}\n",
    "    opsys_map = {'win': -1.0, 'mac': 0.0, 'NA': 0.0, 'tux': 1.0, 'BSD': 1.0}\n",
    "    opensourcer_map = {'never': 0.0, 'year': 0.5, 'month-year': 1.0, 'month': 2.0}\n",
    "\n",
    "    # Convert \"OrgSize\" to log-transformed values\n",
    "    def orgsize_to_log(value):\n",
    "        if value == 'NA':\n",
    "            return 0.0\n",
    "        try:\n",
    "            lower_bound = int(value.split('-')[0].replace(',', ''))\n",
    "            return math.log(lower_bound)\n",
    "        except ValueError:\n",
    "            if value == '10,000+':\n",
    "                return math.log(10000)\n",
    "            return 0.0\n",
    "\n",
    "    # Convert boolean columns ('y', 'n', 'NA') to float (1.0, 0.0, 0.0)\n",
    "    def boolean_to_float(value):\n",
    "        return 1.0 if value == 'y' else 0.0\n",
    "\n",
    "    # Convert columns\n",
    "    data['CareerSat'] = data['CareerSat'].map(career_sat_map).astype(float)\n",
    "    data['MgrWant'] = data['MgrWant'].apply(boolean_to_float)\n",
    "    data['Age'] = data['Age'].replace('NA', '0').astype(float)\n",
    "    data['CodeRevHrs'] = data['CodeRevHrs'].replace('NA', '0').astype(float)\n",
    "    data['ConvertedComp'] = data['ConvertedComp'].replace('NA', '0').astype(float)\n",
    "    data['Dependents'] = data['Dependents'].apply(boolean_to_float)\n",
    "    data['DevEnvironVSC'] = data['DevEnvironVSC'].apply(boolean_to_float)\n",
    "    data['DevTypeFullStack'] = data['DevTypeFullStack'].apply(boolean_to_float)\n",
    "    data['EdLevel'] = data['EdLevel'].map(ed_level_map).astype(float)\n",
    "    data['EduOtherMOOC'] = data['EduOtherMOOC'].apply(boolean_to_float)\n",
    "    data['Extraversion'] = data['Extraversion'].apply(boolean_to_float)\n",
    "    data['GenderIsMan'] = data['GenderIsMan'].apply(boolean_to_float)\n",
    "    data['Hobbyist'] = data['Hobbyist'].apply(boolean_to_float)\n",
    "    data['MgrIdiot'] = data['MgrIdiot'].map(mgr_idiot_map).astype(float)\n",
    "    data['OpSys'] = data['OpSys'].map(opsys_map).astype(float)\n",
    "    data['OpenSourcer'] = data['OpenSourcer'].map(opensourcer_map).astype(float)\n",
    "    data['OrgSize'] = data['OrgSize'].apply(orgsize_to_log)\n",
    "    data['Student'] = data['Student'].apply(boolean_to_float)\n",
    "    data['UndergradMajorIsComputerScience'] = data['UndergradMajorIsComputerScience'].apply(boolean_to_float)\n",
    "    data['UnitTestsProcess'] = data['UnitTestsProcess'].apply(boolean_to_float)\n",
    "    data['WorkWeekHrs'] = data['WorkWeekHrs'].replace('NA', '0').astype(float)\n",
    "    data['YearsCode'] = data['YearsCode'].replace('NA', '0').astype(float)\n",
    "    data['YearsCodePro'] = data['YearsCodePro'].replace('NA', '0').astype(float)\n",
    "\n",
    "    # Drop ignored columns\n",
    "    ignored_columns = ['Country', 'EduOtherSelf', 'Respondent']\n",
    "    data = data.drop(columns=ignored_columns)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Test the function (sample file path provided for clarity)\n",
    "# df = parse_stackoverflow_data(\"eggs.csv.gz\")\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rJW5Um0YAeT"
   },
   "source": [
    "### Q2 Splitting Data\n",
    "\n",
    "Now we prepare the converted data for regression. In this step, we:\n",
    "\n",
    " 1. Extract the data as a numpy array\n",
    " 2. Split the data into train and validation sets.  You can use the first 20% of the data (rounded down) as the validation set and keep the remaining as the training set. (Note that it is common practice to randomize the dataset; this has already been done. Don't shuffle the dataset for this assignment.)\n",
    " 3. Split each set into the predicted column (the first column in the data frame) and the feature columns (the remaining columns), plus a final column corresponding to a constant 1.0 value.  Not that you should keep the ordering of the feature columns the same as they appear in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wuwKKD3YAeU"
   },
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    \"\"\"\n",
    "    Split the data into training and validation sets, and convert them to np.ndarray.\n",
    "\n",
    "    Args:\n",
    "        df: pandas.DataFrame -- the parsed data, as returned by parse_stackoverflow_data()\n",
    "\n",
    "    Returns:\n",
    "        X_train, y_train, X_val, y_val:\n",
    "          - X_train: np.ndarray -- the second 80% of the data features\n",
    "          - y_train: np.ndarray -- the second 80% of the target values\n",
    "          - X_val: np.ndarray -- the first 20% (rounded down) of the data features\n",
    "          - y_val: np.ndarray -- the first 20% of the target values\n",
    "    \"\"\"\n",
    "    # Convert DataFrame to numpy array\n",
    "    data = df.to_numpy()\n",
    "\n",
    "    # Determine split index\n",
    "    split_index = len(data) // 5  # 20% of the data\n",
    "\n",
    "    # Split into validation and training sets\n",
    "    val_data = data[:split_index]\n",
    "    train_data = data[split_index:]\n",
    "\n",
    "    # Separate features (columns 1 onward) and target (column 0)\n",
    "    X_val = val_data[:, 1:]\n",
    "    y_val = val_data[:, 0]\n",
    "    X_train = train_data[:, 1:]\n",
    "    y_train = train_data[:, 0]\n",
    "\n",
    "    # Add a constant column of 1.0 to features for the bias term\n",
    "    X_val = np.hstack((X_val, np.ones((X_val.shape[0], 1))))\n",
    "    X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1))))\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "# Testing with example data would require an appropriate DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdDS-CxBYAec"
   },
   "source": [
    "### Q3 Linear Regression\n",
    "\n",
    "Now we are going to build a simple scikit-learn-like class for least squares linear regression.  Recall from lecture that the linear regression approach models the data as:\n",
    "$$ y^{(i)} \\approx \\theta^T x^{(i)} $$\n",
    "and the optimal $\\theta$ is given by:\n",
    "$$ \\theta^* = (X^T X)^{-1}X^T y $$\n",
    "using the notation described in the slides and course notes.  Recall, as mentioned in class, that you should use the `np.linalg.solve()` function rather than the `np.linalg.inv()` function to compute this solution.\n",
    "\n",
    "Implement the class below, plus the `squared_error` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VKh1SDVYAec"
   },
   "outputs": [],
   "source": [
    "def squared_error(y_pred, y):\n",
    "    \"\"\"\n",
    "    Compute the average squared error between predictions and ground truth.\n",
    "\n",
    "    Args:\n",
    "        y_pred: np.ndarray[num_examples] -- the predictions\n",
    "        y: np.ndarray[num_examples] -- the ground truth values\n",
    "\n",
    "    Returns:\n",
    "        float: Average squared error between y_pred and y.\n",
    "    \"\"\"\n",
    "    return np.mean((y_pred - y) ** 2)\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Perform linear regression and predict the output on unseen examples.\n",
    "\n",
    "    Attributes:\n",
    "        theta (np.ndarray): Vector containing parameters for the features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the linear regression model by computing the estimate of the parameters.\n",
    "\n",
    "        Args:\n",
    "            X: np.ndarray[num_examples, num_columns] -- Matrix of training data.\n",
    "            y: np.ndarray[num_examples] -- Vector of output variables.\n",
    "        \"\"\"\n",
    "        # Compute theta using the closed-form solution (normal equation)\n",
    "        self.theta = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the learned model to predict the output of X.\n",
    "\n",
    "        Args:\n",
    "            X: np.ndarray[num_examples, num_columns] -- Matrix of features for prediction.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray[num_examples]: Vector of predicted outputs.\n",
    "        \"\"\"\n",
    "        return X @ self.theta\n",
    "\n",
    "\n",
    "# Example usage (assuming X_train, y_train, X_val, y_val are prepared):\n",
    "# model = LinearRegression(X_train, y_train)\n",
    "# predictions = model.predict(X_val)\n",
    "# error = squared_error(predictions, y_val)\n",
    "# print(\"Validation Error:\", error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R26vwwiyYAfG"
   },
   "source": [
    "## Q4 Evaluation versus baselines\n",
    "\n",
    "As a final consideration, If you implement this properly, you will see that we get a squared error of around 1.3 on the validation set.  Is this \"good\"?  This is one of the more subtle points of many data science problems, but we can start to get some sense of this by looking at what the predictions would look like if we _just_ predicted the mean target output on the training set (this is essentially the \"simplest\" prediction we could make if we didn't look at the features at all).\n",
    "\n",
    "Implement the following function to evaluate our linear regression.  Think about what this signifies about the quality of the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iekxW7-YAfH"
   },
   "outputs": [],
   "source": [
    "def evaluate_linear_regression(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Evaluate the squared error of linear regression versus the simple mean-prediction baseline.\n",
    "\n",
    "    Args:\n",
    "        X_train: np.ndarray -- Training feature matrix (num_examples, num_columns)\n",
    "        y_train: np.ndarray -- Training target values (num_examples,)\n",
    "        X_val: np.ndarray -- Validation feature matrix (num_examples, num_columns)\n",
    "        y_val: np.ndarray -- Validation target values (num_examples,)\n",
    "\n",
    "    Returns:\n",
    "        Tuple[validation_mse, baseline_mse]:\n",
    "            validation_mse: float -- Squared error of predictions on validation set, when training on training set.\n",
    "            baseline_mse: float -- Squared error of predicting the mean of the training set on the validation set.\n",
    "    \"\"\"\n",
    "    # Train the LinearRegression model\n",
    "    model = LinearRegression(X_train, y_train)\n",
    "    predictions = model.predict(X_val)\n",
    "    \n",
    "    # Compute validation mean squared error for the model\n",
    "    validation_mse = squared_error(predictions, y_val)\n",
    "\n",
    "    # Compute baseline mean squared error (predicting the mean of y_train)\n",
    "    baseline_prediction = np.mean(y_train)\n",
    "    baseline_mse = squared_error(np.full_like(y_val, baseline_prediction), y_val)\n",
    "\n",
    "    return validation_mse, baseline_mse\n",
    "\n",
    "\n",
    "# Example usage (requires prepared data):\n",
    "# validation_mse, baseline_mse = evaluate_linear_regression(X_train, y_train, X_val, y_val)\n",
    "# print(\"Validation MSE:\", validation_mse)\n",
    "# print(\"Baseline MSE:\", baseline_mse)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
